#Step 1: Setup API keys for Grok, OpenAI and Tavily

import os
from dotenv import load_dotenv

# Load environment variables from the .env file
load_dotenv()

#GROK_API_KEY = os.environ.get("GROK_API_KEY")
TAVILY_API_KEY = os.environ.get("TAVILY_API_KEY")
OPENAI_API_KEY = os.environ.get("OPENAI_API_KEY")
GROK_API_KEY = os.environ.get("GROK_API_KEY")

#print(GROK_API_KEY) 

#Step 2: set llms and tools

#imports llm classes using langchain framework
from langchain_groq import ChatGroq
from langchain_openai import ChatOpenAI
from langchain_community.tools.tavily_search import TavilySearchResults


#Instantiating two language model (LLM) objects using the ChatOpenAI and ChatGroq classes, respectively. 
#These objects represent configurations for interacting with specific LLMs provided by OpenAI and Groq.
#openai_llm=ChatOpenAI(model="gpt-4o-mini",api_key=os.getenv("OPENAI_API_KEY"))
#groq_llm=ChatGroq(model="llama-3.3-70b-versatile",api_key=os.getenv("GROK_API_KEY"))
#gets max of best 2 internet search results
search_tool=TavilySearchResults(max_results=2)

#Step 3: setup AI agent with search tool functionality

#create_react_agent function simplifies creating a ReAct (Reasoning + Action) agent,
#which combines logical reasoning with the ability to use tools dynamically
from langgraph.prebuilt import create_react_agent
from langchain_core.messages.ai import AIMessage #imports AIMessage class, a module that represents a message generated by an AI model 

#guides the behavior of the agent
system_prompt = "Act as an AI agent who is smart and friendly"

def get_response_from_ai_agent(llm_id, query, allow_search, system_prompt, provider):
    if provider=="Groq":
        llm=ChatGroq(model=llm_id,api_key=os.getenv("GROK_API_KEY"))
    elif provider=="OpenAI":
        llm=ChatOpenAI(model=llm_id,api_key=os.getenv("OPENAI_API_KEY"))

    if allow_search:
        tools = [search_tool]
    else:
        tools=[]

    agent = create_react_agent(
        model=llm, #underlying LLM, which is the reasoning engine responsible for processing user queries and generating intelligent responses
        tools=tools, #Adds search_tool which allows the agent to interact with external systems to fetch data
        prompt=system_prompt
    )

    #testing to see if the agent works
    #query = "Tell me about the trends in crypto markets"
    state = {"messages":query}

    response=agent.invoke(state)
    """
    The response dictionary contains a key "messages", which maps to a list of message objects. 
    # Each object represents a message, with details about its type (e.g., HumanMessage, AIMessage, ToolMessage), 
    # content, metadata, and any additional information relevant to the interaction.
    """
    messages = response.get("messages")

    #Extracts the messages list from the response.
    #Filters for AIMessage objects, extracting their content attribute.
    ai_messages = [message.content for message in messages if isinstance(message, AIMessage)]
    #print(type(response))
    #print(response) # returns specialized dict-like object from the langgraph.pregel.io module which has all the data including metadata which is not needed for us
    return ai_messages[-1] # return the last ai message content
